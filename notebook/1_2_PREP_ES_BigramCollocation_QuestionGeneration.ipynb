{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890431d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "#Default\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "def reset(df):\n",
    "    cols = df.columns\n",
    "    return df.reset_index()[cols]\n",
    "def print_counts(df):\n",
    "    cols = df.columns\n",
    "    for each in cols:\n",
    "        print(each)\n",
    "        print(df[each].value_counts())\n",
    "        print('______________________________________')\n",
    "# ~\n",
    "#Default Ending\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01f83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_dicts, fetch_archive_from_http, print_answers\n",
    "from haystack.nodes import DensePassageRetriever, EmbeddingRetriever, FARMReader, EntityExtractor, QuestionGenerator\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "import torch\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5656cf0",
   "metadata": {},
   "source": [
    "# Embedding Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409ddfad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For EmbeddingRetriever\n",
    "similarity_type = \"cosine\"\n",
    "\n",
    "container_name = \"localhost\"\n",
    "\n",
    "er_co = ElasticsearchDocumentStore(host=container_name, port = \"9200\", index=\"production_er_bigram_collocation\",\n",
    "                                           similarity=similarity_type, embedding_dim=768)\n",
    "\n",
    "er_200 = ElasticsearchDocumentStore(host=container_name, port = \"9200\", index=\"production_er_200\",\n",
    "                                           similarity=similarity_type, embedding_dim=768)\n",
    "\n",
    "er_qg = ElasticsearchDocumentStore(host=container_name, port = \"9200\", index=\"production_er_question_generation\",\n",
    "                                           similarity=similarity_type, embedding_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25186bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pdftotext version 4.03 [www.xpdfreader.com]\n",
      "Copyright 1996-2021 Glyph & Cog, LLC\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import TextConverter, PDFToTextConverter, DocxToTextConverter, PreProcessor\n",
    "\n",
    "converter = PDFToTextConverter(remove_numeric_tables=False, valid_languages = [\"en\"])\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604e32e",
   "metadata": {},
   "source": [
    "# Set document_store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca25b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = er_co\n",
    "document_store_qg_input = er_200\n",
    "document_store_qg = er_qg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d8bb3",
   "metadata": {},
   "source": [
    "# Process Raw PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220d9bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "def clean_content(content):\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('..','.')\n",
    "    content = content.replace('\\n',' ')\n",
    "    content = content.replace('\\n',' ')\n",
    "    content = \" \".join(content.split())\n",
    "    return content\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79f9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('data/valid_file_list.pickle', 'rb') as handle:\n",
    "#     file_list = pickle.load(handle)\n",
    "    \n",
    "# len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc016d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(file_list)\n",
    "\n",
    "# file_list = [each for each in file_list if 'OCR' not in each]\n",
    "\n",
    "# len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887f5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['RIA_Input/03-BCBS/risk/The role of household debt heterogeneity on consumption_ Evidence from Japanese household data.pdf']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcecc1",
   "metadata": {},
   "source": [
    "# Clean Text & NER Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86a5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def tokenize(sentences):\n",
    "    for sent in nltk.sent_tokenize(sentences.lower()):\n",
    "        for word in nltk.word_tokenize(sent):                 \n",
    "            yield word\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66af9052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precautionary saving; annual change; debt ratio; household debt; debt\n",
      "heterogeneity; income elasticity; saving motive; low high; regression\n",
      "model; old age; saving motives; monetary policy; negative income;\n",
      "highly-indebted households; income change; international settlements;\n",
      "year-fixed effects; categorical choices; standard errors; control\n",
      "variables; 0.0 low; japanese households; dependent variable; robust\n",
      "standard; statistical significance; june 2018; indicate statistical;\n",
      "pps dataset; panel regression; estimation results; parameters study;\n",
      "liquidity constraints; balance sheet; economic activity; residence\n",
      "change; july 2018; public pension; zero otherwise; economic review;\n",
      "household size; consumption behaviour; estimated using; high low;\n",
      "22,485 5,837; jouchi nakajima; income changes; future work;\n",
      "significantly positive; debt burdens; see also; job change; financial\n",
      "stability; highly indebted; portfolio selection; upper quartile;\n",
      "regression result; preference parameters; future unemployment; working\n",
      "papers; observations households; marginal propensities; dataset\n",
      "includes; 0.2 0.1; bis working; size job; quarterly journal; financial\n",
      "assets; dummy variable; latter stages; working paper; robustness\n",
      "check; precautionary savings; 21,018 5,620; andrew filardo; basic\n",
      "residents; concluding remarks; dubravko mihaljek; econometric\n",
      "methodology; g11 keywords; hyun song; intertemporal substitution; jel\n",
      "classification; least squares; ordinary least; residents registration;\n",
      "song shin; marginal propensity; monetary economics; may 2018; american\n",
      "economic; average propensity; next-year income; using ols; japanese\n",
      "yen; cross-sectional averages; osaka university; rate mortgages;\n",
      "registration system; stefan avdjiev; views expressed; debt service;\n",
      "0.3 0.8; 0.8 0.2; business cycles; long run; numeric assignment;\n",
      "little difference; political economy; preferences parameters;\n",
      "empirical evidence; household members; credit constraints; residential\n",
      "area; balance sheets; forward-looking questions; mortality risk; open-\n",
      "ended categories; summary statistics; consumption patterns; japanese\n",
      "household; coefficient measures; nber working; wealth effects;\n",
      "category range; brookings papers; pension restructuring; household\n",
      "balance; indebted households; substitution effect; households tend;\n",
      "using ordinary; change residence; unexpected change; 0.3 0.2; ages\n",
      "36-50; dataset confirms; debt burden; paper assesses; 0.4 0.3; asian\n",
      "countries; household-level data; micro data; policy effectiveness;\n",
      "survey asks; expected change; 0.1 0.0; explanatory variable; control\n",
      "variable; saving behaviour; economic department; provide empirical;\n",
      "sheet information; interest rates; vol 101; lower level; previous\n",
      "year; dummy -0.038; household leverage; reports estimation; current\n",
      "year; change observations; unemployment spells; income shocks; result\n",
      "implies; discussion paper; paper series; global financial; variables\n",
      "change; unemployment risks; bis quarterly; job categories; quarterly\n",
      "review; age cohorts; numeric variable; mpc across; unemployment\n",
      "insurance; motives due; middle-aged households; no-debt households;\n",
      "younger households; significantly higher; policy implications; income\n",
      "uncertainty; unemployment risk; debt levels; wealth effect; results\n",
      "indicate; national income; positive income; debt overhang; household\n",
      "data; different age; unexpected changes; household survey; saving\n",
      "hypothesis; baseline regression; expected changes; important role;\n",
      "expenditure growth; savings behaviour; highlyindebted households;\n",
      "households typically; panel data; behaviour using; household\n",
      "characteristics; following regression; high levels; variables table;\n",
      "policy uncertainty; debt ratios; average annual; survey data;\n",
      "regression analysis; income relative; mortgage debt; japanese dataset;\n",
      "consumption growth; across households; households face; holding debt;\n",
      "precautionary motives; annual changes; housing debt; debt tend;\n",
      "household savings; different debt; expected income; income increase;\n",
      "consumption expenditure; consumption regression; annual income; low\n",
      "debt; elasticity income; household consumption; income debt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_list = []\n",
    "for file_name in tqdm(file_list):\n",
    "    doc = converter.convert(file_path = file_name, meta = None)\n",
    "    long_text = doc[0]['content']\n",
    "    text = nltk.Text(tkn for tkn in tokenize(long_text))\n",
    "    text.collocations(num=10000000, window_size = 2)\n",
    "    collocations = [\" \".join(el) for el in list(text._collocations)]\n",
    "    collocations = list(set(collocations))\n",
    "    \n",
    "    keyword = file_name.split('/')[-2]\n",
    "    central_bank = file_name.split('/')[-3]\n",
    "    \n",
    "    for each_col in collocations:\n",
    "        doc_dict = {}\n",
    "        doc_dict['content'] = each_col\n",
    "        doc_dict['content_type'] = 'text'\n",
    "        doc_dict['meta'] = {}\n",
    "        doc_dict['meta']['keyword'] = keyword\n",
    "        doc_dict['meta']['central_bank'] = central_bank\n",
    "        doc_list.append(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6a7010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 3.34 ms, total: 26.1 ms\n",
      "Wall time: 2.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "document_store.delete_documents()\n",
    "\n",
    "document_store.write_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c5bec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model all-mpnet-base-v1\n",
      "INFO - haystack.document_stores.elasticsearch -  Updating embeddings for all 235 docs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa966417ee545c0a30c3e7ab9ae64f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating embeddings:   0%|          | 0/235 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d9bb823907442dad9563de4faf63a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "er_retriever = EmbeddingRetriever(\n",
    "   document_store=document_store,\n",
    "   embedding_model=\"all-mpnet-base-v1\",\n",
    "   model_format=\"sentence_transformers\",\n",
    "   use_gpu = True,\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(er_retriever)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404ad8f",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720636ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "from haystack.pipelines import QuestionGenerationPipeline\n",
    "\n",
    "question_generator = QuestionGenerator(model_name_or_path = 't5-base-e2e-qg')\n",
    "\n",
    "question_generation_pipeline = QuestionGenerationPipeline(question_generator)\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0a8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:52, 26.40s/it]\n"
     ]
    }
   ],
   "source": [
    "doc_list = []\n",
    "for idx, document in tqdm(enumerate(document_store_qg_input)):\n",
    "    keyword = document.__dict__['meta']['keyword']\n",
    "    central_bank = document.__dict__['meta']['central_bank']\n",
    "    result = question_generation_pipeline.run(documents=[document])\n",
    "    for each_qg in result['generated_questions'][0]['questions']:\n",
    "        doc_dict = {}\n",
    "        doc_dict['content'] = each_qg\n",
    "        doc_dict['content_type'] = 'text'\n",
    "        doc_dict['meta'] = {}\n",
    "        doc_dict['meta']['keyword'] = keyword\n",
    "        doc_dict['meta']['central_bank'] = central_bank\n",
    "        doc_list.append(doc_dict)\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70eb52b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.99 ms, sys: 2.76 ms, total: 12.8 ms\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "document_store_qg.delete_documents()\n",
    "\n",
    "document_store_qg.write_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f308493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model all-mpnet-base-v1\n",
      "INFO - haystack.document_stores.elasticsearch -  Updating embeddings for all 38 docs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396fd0e3059247aaa683aa46ba54c885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating embeddings:   0%|          | 0/38 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c79027a8064a5997ce6db6706eeccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "er_retriever = EmbeddingRetriever(\n",
    "   document_store=document_store_qg,\n",
    "   embedding_model=\"all-mpnet-base-v1\",\n",
    "   model_format=\"sentence_transformers\",\n",
    "   use_gpu = True,\n",
    ")\n",
    "\n",
    "document_store_qg.update_embeddings(er_retriever)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807e823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38hay]",
   "language": "python",
   "name": "conda-env-py38hay-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
