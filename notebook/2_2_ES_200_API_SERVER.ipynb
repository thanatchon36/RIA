{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890431d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "#Default\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "def reset(df):\n",
    "    cols = df.columns\n",
    "    return df.reset_index()[cols]\n",
    "def print_counts(df):\n",
    "    cols = df.columns\n",
    "    for each in cols:\n",
    "        print(each)\n",
    "        print(df[each].value_counts())\n",
    "        print('______________________________________')\n",
    "        \n",
    "def flatten(list_1):\n",
    "    list_2 = []\n",
    "    for each in list(list_1):\n",
    "        try:\n",
    "            for each_2 in each:\n",
    "                list_2.append(each_2)\n",
    "        except:\n",
    "            pass\n",
    "    return list_2\n",
    "        \n",
    "# ~\n",
    "#Default Ending\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01f83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_dicts, fetch_archive_from_http, print_answers\n",
    "from haystack.nodes import DensePassageRetriever, EmbeddingRetriever, FARMReader, EntityExtractor\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.pipelines import ExtractiveQAPipeline, DocumentSearchPipeline\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5656cf0",
   "metadata": {},
   "source": [
    "# Set similarity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ed70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DensePassageRetriever\n",
    "similarity_type = \"dot_product\"\n",
    "\n",
    "# For EmbeddingRetriever\n",
    "# similarity_type = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409ddfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store_100 = ElasticsearchDocumentStore(host=\"localhost\", port = \"9200\", index=\"document_100\",\n",
    "                                           similarity=similarity_type)\n",
    "document_store_200 = ElasticsearchDocumentStore(host=\"localhost\", port = \"9200\", index=\"document_200\",\n",
    "                                           similarity=similarity_type)\n",
    "document_store_300 = ElasticsearchDocumentStore(host=\"localhost\", port = \"9200\", index=\"document_300\",\n",
    "                                           similarity=similarity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604e32e",
   "metadata": {},
   "source": [
    "# Set document_store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca25b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = document_store_200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02601e8a",
   "metadata": {},
   "source": [
    "# Set Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085264a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at DPR-retriever/query_encoder\n",
      "INFO - haystack.modeling.model.language_model -  Loaded DPR-retriever/query_encoder\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at DPR-retriever/passage_encoder\n",
      "INFO - haystack.modeling.model.language_model -  Loaded DPR-retriever/passage_encoder\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"DPR-retriever/query_encoder\",\n",
    "    passage_embedding_model=\"DPR-retriever/passage_encoder\",\n",
    "    max_seq_len_query=64,\n",
    "    max_seq_len_passage=256,\n",
    "    batch_size=16,\n",
    "    use_gpu=True,\n",
    "    embed_title=True,\n",
    "    use_fast_tokenizers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f96b9",
   "metadata": {},
   "source": [
    "# Set Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ada741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at minilm-uncased-squad2-reader\n",
      "INFO - haystack.modeling.model.language_model -  Loaded minilm-uncased-squad2-reader\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from minilm-uncased-squad2-reader/prediction_head_0.bin\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='prediction_heads' was already logged with value='TextSimilarityHead' for run ID='91c70ad73c5e49989ddd7167606429c5'. Attempted logging new value 'QuestionAnsweringHead'.\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='processor' was already logged with value='TextSimilarityProcessor' for run ID='91c70ad73c5e49989ddd7167606429c5'. Attempted logging new value 'SquadProcessor'.\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 7 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0    0    0    0    0    0    0 \n",
      "INFO - haystack.modeling.infer -  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "INFO - haystack.modeling.infer -  /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n"
     ]
    }
   ],
   "source": [
    "# reader = FARMReader(model_name_or_path=\"albert_xxlargev1_squad2_512-reader\", use_gpu=True,\n",
    "#                    context_window_size = 500)\n",
    "\n",
    "# reader = FARMReader(model_name_or_path=\"roberta-base-squad2-reader\", use_gpu=True,\n",
    "#                    context_window_size = 500)\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"minilm-uncased-squad2-reader\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0d78e",
   "metadata": {},
   "source": [
    "# Set Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc9ea4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_pipe = DocumentSearchPipeline(retriever)\n",
    "reader_pipe = ExtractiveQAPipeline(reader,retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff166104",
   "metadata": {},
   "source": [
    "# Set Meta Filter Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4f788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_list = list(document_store)\n",
    "meta_list = [each.__dict__['meta'] for each in meta_list]\n",
    "meta_df = pd.DataFrame(meta_list)\n",
    "\n",
    "# By Sentence\n",
    "main_col_list = ['central_bank', 'keyword']\n",
    "\n",
    "# Editing this will depend on NER models\n",
    "alt_col_list = ['ORG', 'LOC', 'PER', 'MISC']\n",
    "\n",
    "fil_col_list = main_col_list + alt_col_list\n",
    "min_count = 3\n",
    "meta_fil_dict = {}\n",
    "for col in fil_col_list:\n",
    "    if col in alt_col_list and col in list(meta_df.columns):\n",
    "        temp_df = pd.DataFrame(flatten(list(meta_df[col].values)))[0].value_counts().reset_index()\n",
    "    elif col in main_col_list and col in list(meta_df.columns):\n",
    "        temp_df = pd.DataFrame(list(meta_df[col].values))[0].value_counts().reset_index()\n",
    "    \n",
    "    if col in list(meta_df.columns):\n",
    "        temp_df = temp_df[temp_df[0] >= min_count]\n",
    "        temp_dict = dict(zip(temp_df['index'], temp_df[0]))\n",
    "        meta_fil_dict[col] = temp_dict\n",
    "        \n",
    "return_meta_fil_dict = {}\n",
    "return_meta_fil_dict['sentence'] = meta_fil_dict\n",
    "\n",
    "# By Document files\n",
    "meta_df = meta_df.groupby('file_name').first().reset_index()\n",
    "min_count = 0\n",
    "fil_col_list = ['central_bank', 'keyword']\n",
    "meta_fil_dict = {}\n",
    "for col in fil_col_list:\n",
    "    if col in alt_col_list and col in list(meta_df.columns):\n",
    "        temp_df = pd.DataFrame(flatten(list(meta_df[col].values)))[0].value_counts().reset_index()\n",
    "    elif col in main_col_list and col in list(meta_df.columns):\n",
    "        temp_df = pd.DataFrame(list(meta_df[col].values))[0].value_counts().reset_index()\n",
    "    \n",
    "    if col in list(meta_df.columns):\n",
    "        temp_df = temp_df[temp_df[0] >= min_count]\n",
    "        temp_dict = dict(zip(temp_df['index'], temp_df[0]))\n",
    "        meta_fil_dict[col] = temp_dict\n",
    "        \n",
    "return_meta_fil_dict['document'] = meta_fil_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d195fc8",
   "metadata": {},
   "source": [
    "# Set API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df651c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/haystack_200_reader_pipe', methods=['POST'])\n",
    "def haystack_200_reader_pipe():\n",
    "    query_list = []\n",
    "    if request.method == 'POST':\n",
    "        body = request.get_json()        \n",
    "        prediction = reader_pipe.run(\n",
    "            query=body['query'], params={\"Retriever\": {\"top_k\": int(body['top_k_retriever'])},\n",
    "                                         \"Reader\": {'top_k': int(body['top_k_reader'])},\n",
    "                                         \"filters\": body['filters'],\n",
    "                                         }\n",
    "        )\n",
    "        ans_dict = prediction['answers']\n",
    "        ans_dict = [each.__dict__ for each in ans_dict]\n",
    "        prediction['answers'] = ans_dict\n",
    "        doc_dict = prediction['documents']\n",
    "        doc_dict = [each.__dict__ for each in doc_dict]\n",
    "        prediction['documents'] = doc_dict\n",
    "        \n",
    "        return prediction, 201\n",
    "    \n",
    "@app.route('/haystack_200_retriever_pipe', methods=['POST'])\n",
    "def haystack_200_retriever_pipe():\n",
    "    query_list = []\n",
    "    if request.method == 'POST':\n",
    "        body = request.get_json()        \n",
    "        prediction = retriever_pipe.run(\n",
    "            query=body['query'], params={\"Retriever\": {\"top_k\": int(body['top_k_retriever'])},\n",
    "                                         \"filters\": body['filters'],\n",
    "                                         }\n",
    "        )\n",
    "        doc_list = [each.__dict__ for each in prediction['documents']]\n",
    "        prediction['documents'] = doc_list\n",
    "        return prediction, 201\n",
    "    \n",
    "@app.route('/haystack_200_meta', methods=['GET'])\n",
    "def haystack_200_meta():\n",
    "    query_list = []\n",
    "    if request.method == 'GET':        \n",
    "        return return_meta_fil_dict, 201\n",
    "    \n",
    "print('OK !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c0d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - werkzeug -   * Running on http://127.0.0.1:5201/ (Press CTRL+C to quit)\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|                        | 0/1 [00:00<?, ? Batches/s]/opt/anaconda3/envs/py38hay/lib/python3.8/site-packages/haystack/modeling/model/prediction_head.py:485: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:01<00:00,  1.24s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:01<00:00,  1.44s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:01<00:00,  1.33s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:01<00:00,  1.33s/ Batches]\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:37:19] \"\u001b[35m\u001b[1mPOST /haystack_200_reader_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:37:35] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:37:40] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:37:44] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:37:46] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:38:59] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:40:30] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:40:31] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:42:37] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:42:40] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:42:43] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:43:15] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:43:20] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:43:25] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:43:41] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:43:48] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:01] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:08] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:11] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:15] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:20] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:25] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:29] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:47] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:48] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:44:56] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:45:04] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:45:30] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:45:42] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:45:45] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:45:56] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:46:04] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.49 Batches/s]\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:46:32] \"\u001b[35m\u001b[1mPOST /haystack_200_reader_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.83 Batches/s]\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:47:11] \"\u001b[35m\u001b[1mPOST /haystack_200_reader_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:53:24] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:56:56] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:57:51] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 14:59:56] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:00:02] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:00:13] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:00:22] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:00:23] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:01<00:00,  1.01s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:04<00:00,  4.33s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:01<00:00,  1.10s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.63 Batches/s]\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:02:36] \"\u001b[35m\u001b[1mPOST /haystack_200_reader_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:04<00:00,  4.07s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 2/2 [00:21<00:00, 10.94s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.79 Batches/s]\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:03:10] \"\u001b[35m\u001b[1mPOST /haystack_200_reader_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:04:23] \"\u001b[35m\u001b[1mGET /haystack_200_meta HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:06:06] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:06:08] \"\u001b[35m\u001b[1mPOST /haystack_200_retriever_pipe HTTP/1.1\u001b[0m\" 201 -\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:04<00:00,  4.06s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  1.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 2/2 [00:22<00:00, 11.28s/ Batches]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████| 1/1 [00:00<00:00,  2.81 Batches/s]\n",
      "INFO - werkzeug -  127.0.0.1 - - [02/Apr/2022 15:06:46] \"\u001b[35m\u001b[1mPOST /haystack_200_reader_pipe HTTP/1.1\u001b[0m\" 201 -\n"
     ]
    }
   ],
   "source": [
    "app.run(port = 5201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3a722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7579b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae609a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e615efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38hay]",
   "language": "python",
   "name": "conda-env-py38hay-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
